---
kind: Pod
apiVersion: v1
metadata:
  name: frr
  namespace: openshift-{{ onPremPlatformShortName .ControllerConfig }}-infra
  deletionGracePeriodSeconds: 65
  labels:
    app: '{{ onPremPlatformShortName .ControllerConfig }}-infra-bgp'
  annotations:
    target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
spec:
  volumes:
  - name: resource-dir
    hostPath:
      path: "/etc/kubernetes/static-pod-resources/frr"
  - name: kubeconfig
    hostPath:
      path: "/etc/kubernetes/kubeconfig"
  - name: conf-dir
    hostPath:
      path: "/etc/frr"
  - name: manifests
    hostPath:
      path: "/opt/openshift/manifests"
  - name: run-dir
    empty-dir: {}
  initContainers:
  # (emilien) We need to create /etc/frr/daemons and /etc/frr/vtysh.conf files because
  # /etc/frr is bind-mounted to generate frr.conf and frr also needs daemons configured.
  # The frr container livenessProbe needs vtysh.conf as well to be able to run.
  # There is no simple way to tell frr.conf where daemons file is and for now we will leave
  # it in /etc/frr.
  - name: copy-frr-static-configs
    image: {{ .Images.BaremetalRuntimeCfgBootstrap }}
    securityContext:
      privileged: true
    command:
    - /bin/bash
    - -c
    - |
      cat <<EOF > /etc/frr/daemons
      # generated by copy-frr-daemons container
      bgpd=yes
      ospfd=no
      ospf6d=no
      ripd=no
      ripngd=no
      isisd=no
      pimd=no
      nhrpd=no
      eigrpd=no
      sharpd=no
      pbrd=no
      bfdd=yes
      fabricd=no
      vrrpd=no
      pathd=no
      vtysh_enable=yes
      zebra_options="  -A 127.0.0.1 -s 90000000"
      bgpd_options="   -A 127.0.0.1"
      ospfd_options="  -A 127.0.0.1"
      ospf6d_options=" -A ::1"
      ripd_options="   -A 127.0.0.1"
      ripngd_options=" -A ::1"
      isisd_options="  -A 127.0.0.1"
      pimd_options="   -A 127.0.0.1"
      nhrpd_options="  -A 127.0.0.1"
      eigrpd_options=" -A 127.0.0.1"
      sharpd_options=" -A 127.0.0.1"
      pbrd_options="   -A 127.0.0.1"
      staticd_options="-A 127.0.0.1"
      bfdd_options="   -A 127.0.0.1"
      fabricd_options="-A 127.0.0.1"
      vrrpd_options="  -A 127.0.0.1"
      pathd_options="  -A 127.0.0.1"
      EOF
      cat <<EOF > /etc/frr/vtysh.conf
      service integrated-vtysh-config
      EOF
    resources: {}
    volumeMounts:
    - name: conf-dir
      mountPath: "/etc/frr"
    imagePullPolicy: IfNotPresent
  containers:
  - name: frr
    # change that, see
    # https://github.com/metallb/metallb/blob/350855a42d4d2bd510c9cdd6d484d49224879436/config/frr/speaker-patch.yaml#L55-L57
    securityContext:
      privileged: true
    image: quay.io/emilien/frr:latest
    command:
    - /bin/bash
    - -c
    - |
      #!/bin/bash
      #
      # TODO in this script:
      # * IPv6 support
      # * test with multiple VIPs
      #
 
      VIPS="{{- range $index, $ip := onPremPlatformAPIServerInternalIPs .ControllerConfig }}{{ if gt $index 0 }} {{end}}{{$ip}}{{end}}"

      GATEWAY=$(ip route | grep default | head -n 1 | awk '{print $3}')
      NETWORK_CIDR=$(ip route | grep "proto kernel scope link src" |  awk '{print $1}')

      check_vip()
      {
          # check if the VIP is assigned to this node.
          # return 0 if it is, 1 if not.
          # takes one argument: the VIP to check.
          local vip=$1
          if ip address show dev lo | grep -q $vip; then
              echo `date "+%FT%T"` "VIP $vip is assigned to this node"
              return 0
          else
              echo `date "+%FT%T"` "VIP $vip is not assigned to this node"
              return 1
          fi
      }
      
      # Since the VIP won't be in the same subnet of the bootstrap / master nodes, we need to configure
      # routing on the host as well. We spent a bit of time investigating it and the problem with the VIP
      # traffic going back to its source (specially for HTTPS) is that the traffic path is asymetric.
      # The destination MAC on the request was different than the source MAC the client was seeing on the reply;
      # and then packet were dropped.
      # We think that we have two options:
      # * Use a specific routing table that will route outgoing traffic coming from the VIP and supposed
      #   to go to the nodes subnet, to go through the gateway so that the traffic back follows the same
      #   path as the traffic in.
      # * For all the hosts in the nodes subnet, provide a direct route to reach the VIP.
      #
      # We decided to go with the first option (VIP traffic goes through router) because the VIP will be
      # active/active on multiple nodes (masters).
      # While this sounds complex, this comes due to abusing the deployment having different subnet IPs
      # on the same segment and expect it to not go through the router, so we need to make symetric traffic.
      create_vip_routing()
      {
          # prepare the host to route the VIP traffic through the gateway
          # takes one argument: the VIP to route.
          local vip=$1
          if ! ip rule show table 168 | grep -q $vip; then
              echo `date "+%FT%T"` "Create ip rule for $NETWORK_CIDR in table 168"
              ip rule add from $vip to $NETWORK_CIDR lookup 168
          fi
          if ! ip route show table 168 | grep -q $GATEWAY; then
              echo `date "+%FT%T"` "Create default route for table 168 via $GATEWAY"
              ip route add default via $GATEWAY table 168
          fi
      }
      
      create_vip()
      {
          # create the VIP on the node.
          # takes one argument: the VIP to create.
          local vip=$1
          if ! ip address show dev lo | grep -q $vip; then
            echo `date "+%FT%T"` "Creating VIP $vip"
            ip address add $vip dev lo
          fi
      }
      
      delete_vip() {
          # delete the VIP on the node.
          # takes one argument: the VIP to delete.
          local vip=$1
          if ip address show dev lo | grep -q $vip; then
            echo `date "+%FT%T"` "Deleting VIP $vip"
            ip address del $vip dev lo
          fi
      }
      
      start_all_vips()
      {
        VIP_NETMASK=32
        echo `date "+%FT%T"` "Starting all VIPs"
        for vip in $VIPS; do
          create_vip $vip/$VIP_NETMASK
          create_vip_routing $vip
        done
      }
        
      stop_all_vips()
      {
        VIP_NETMASK=32
        echo `date "+%FT%T"` "Stopping all VIPs"
        for vip in $VIPS; do
          delete_vip $vip/$VIP_NETMASK
        done
      }
     
      start_frr()
      {
          /usr/libexec/frr/frr start
          start_all_vips
      }

      stop_frr()
      {
        echo `date "+%FT%T"` "Frr process stopped" >> /var/run/frr/stopped
        stop_all_vips
        [ -r /var/run/frr/watchfrr.pid ] && /usr/libexec/frr/frr stop
      }
      
      reload_frr()
      {
        stop_frr
        start_frr
      }
      
      msg_handler()
      {
        while read -r line; do
          echo `date "+%FT%T"` "The client sent: $line" >&2
          # currently only 'stop' messages are supported
          if  [ "$line" = stop ]; then
              stop_frr
          elif  [ "$line" = reload ]; then
              reload_frr
          fi
        done
      }
      
      set -e
      declare -r frr_sock="/var/run/frr/frr.sock"
      export VIPS
      export GATEWAY
      export NETWORK_CIDR
      export -f check_vip
      export -f create_vip_routing
      export -f create_vip
      export -f delete_vip
      export -f start_all_vips
      export -f stop_all_vips
      export -f msg_handler
      export -f reload_frr
      export -f start_frr
      export -f stop_frr
      
      while [ -s "/var/run/frr/stopped" ]; do
         echo `date "+%FT%T"` "Container stopped"
         sleep 60
      done

      # Here we need to wait for baremetal-runtimecfg to generate frr.conf, which can take
      # time since it needs the local kubeapi to run.
      if timeout 5m bash -c "while true; do [ -f /etc/frr/frr.conf ] && break; done"; then
          start_frr
      else
          echo `date "+%FT%T"` "ERROR: /etc/frr/frr.conf was not found"
          exit 1
      fi

      rm -f "$frr_sock"
      socat UNIX-LISTEN:${frr_sock},fork system:'bash -c msg_handler'
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
    volumeMounts:
    - name: conf-dir
      mountPath: "/etc/frr"
    - name: run-dir
      mountPath: "/var/run/frr"
    livenessProbe:
      exec:
        command:
        - /bin/bash
        - -c
        - |
          [[ ! -f /etc/frr/frr.conf ]] || \
          [[ -s /var/run/frr/stopped ]] || \
          vtysh -c "show bfd peers"
      initialDelaySeconds: 20
      timeoutSeconds: 5
    terminationMessagePolicy: FallbackToLogsOnError
    imagePullPolicy: IfNotPresent
  - name: frr-monitor
    image: docker.io/emacchi/baremetal-runtimecfg:bgp
    env:
      - name: IS_BOOTSTRAP
        value: "yes"
    command:
    - dynfrr
    - "/etc/kubernetes/kubeconfig"
    - "/config/frr.conf.tmpl"
    - "/etc/frr/frr.conf"
    - "--api-vips"
    - "{{- range $index, $ip := onPremPlatformAPIServerInternalIPs .ControllerConfig }}{{ if gt $index 0 }},{{end}}{{$ip}}{{end}}"
    - "--ingress-vips"
    - "{{- range $index, $ip := onPremPlatformIngressIPs .ControllerConfig }}{{ if gt $index 0 }},{{end}}{{$ip}}{{end}}"
    - "--cluster-config"
    - "/opt/openshift/manifests/cluster-config.yaml"
    - "--check-interval"
    - "5s"
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
    volumeMounts:
    - name: resource-dir
      mountPath: "/config"
    - name: kubeconfig
      mountPath: "/etc/kubernetes/kubeconfig"
    - name: conf-dir
      mountPath: "/etc/frr"
    - name: run-dir
      mountPath: "/var/run/frr"
    - name: manifests
      mountPath: "/opt/openshift/manifests"
    imagePullPolicy: IfNotPresent
  hostNetwork: true
  tolerations:
  - operator: Exists
  priorityClassName: system-node-critical
status: {}
