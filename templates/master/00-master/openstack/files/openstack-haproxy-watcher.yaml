filesystem: "root"
mode: 0755
path: "/etc/kubernetes/static-pod-resources/haproxy/resources/haproxy-watcher.sh"
contents:
  inline: |
    #!/bin/bash
    set -eux
    set -o pipefail

    update_cfg_and_restart() {
        CHANGED=$(diff /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.new) || true
        if [[ ! -f /etc/haproxy/haproxy.cfg ]] || [[ -n "$CHANGED" ]];
        then
            echo "Haproxy config changed. Reloading:"
            cp /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.backup || true
            cp /etc/haproxy/haproxy.cfg.new /etc/haproxy/haproxy.cfg
            echo "reload" | socat /var/run/haproxy/haproxy-master.sock -
        fi
    }

    # NOTE(mandre) this file is present on the first set of master nodes,
    # created during initial deployment. We're getting the $BOOTSTRAP_IP
    # variable from it.
    if [ -f /etc/kubernetes/static-pod-resources/clustervars ]; then
        source /etc/kubernetes/static-pod-resources/clustervars
    fi

    # FIXME(mandre) sometimes noticed it fails to talk to kube-api because the
    # certs are missing. Not sure if they're not created or deleted. We need to
    # find a more reliable way to query kube-api for nodes.
    # KNI gets the address of the master nodes by querying the SRV records for
    # "_etcd-server-ssl._tcp.$domain"
    # https://github.com/openshift-metal3/dev-scripts/blob/master/assets/files/etc/kubernetes/static-pod-resources/haproxy/utils.sh#L3
    export KUBECONFIG=/var/lib/kubelet/kubeconfig

    rules=$(iptables -L PREROUTING -n -t nat --line-numbers | awk '/OCP_API_LB_REDIRECT/ {print $1}'  | tac)
    if [[ -z "$rules" ]]; then
        # FIXME(mandre) Get the cluster CIDR block from the installer
        # This would be even better to put this rule directly in terraform or ignition
        iptables -t nat -I PREROUTING ! --src 10.128.0.0/14 --dst 0/0 -p tcp --dport "$api_port" -j REDIRECT --to-ports "$lb_port" -m comment --comment "OCP_API_LB_REDIRECT"
    fi

    TEMPLATE="{{`{{range .items}}{{\$addresses:=.status.addresses}}{{range .status.conditions}}{{if eq .type \"Ready\"}}{{if eq .status \"True\" }}{{range \$addresses}}{{if eq .type \"InternalIP\"}}{{.address}}{{end}}{{end}}{{end}}{{end}}{{end}} {{end}}`}}"
    while true; do
        WORKERS=
        WORKER_LINES=
        echo "Trying to get worker nodes from OpenShift"
        if [ -f /var/lib/kubelet/pki/kubelet-client-current.pem ]; then
            WORKERS=$(/bin/oc get nodes -l node-role.kubernetes.io/worker -ogo-template="$TEMPLATE")
        fi

        cat > /etc/haproxy/haproxy.cfg.new << EOF
    # NOTE(shadower): this is an haproxy.cfg GENERATED by the haproxy-watcher script
    listen {{.EtcdDiscoveryDomain}}-api-masters
        bind 0.0.0.0:7443
        mode tcp
        balance roundrobin
        server lb-api-vip-6443 {{ .Infra.Status.PlatformStatus.OpenStack.APIServerInternalIP }} check port 6443
    EOF

        if [[ -n "$WORKERS" ]]; then
            for worker in $WORKERS;
            do
                WORKER_LINES="$WORKER_LINES
        server $worker $worker check port 443"
            done
            cat >> /etc/haproxy/haproxy.cfg.new << EOF
    listen {{.EtcdDiscoveryDomain}}-api-workers
        bind 0.0.0.0:80
        bind 0.0.0.0:443
        mode tcp
        balance roundrobin$WORKER_LINES
    EOF
        else
            echo "No worker nodes were found."
        fi
        update_cfg_and_restart
        # Wait for half a minute
        sleep 30
    done
