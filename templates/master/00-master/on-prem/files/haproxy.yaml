mode: 0644
path: {{ if isOpenShiftManagedDefaultLB . -}} "/etc/kubernetes/manifests/haproxy.yaml" {{ else }} "/etc/kubernetes/disabled-manifests/haproxy.yaml" {{ end }}
contents:
  inline: |
    kind: Pod
    apiVersion: v1
    metadata:
      name: haproxy
      namespace: openshift-{{ onPremPlatformShortName . }}-infra
      creationTimestamp:
      deletionGracePeriodSeconds: 65
      labels:
        app: {{ onPremPlatformShortName . }}-infra-api-lb
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
        openshift.io/required-scc: privileged
    spec:
      volumes:
      - name: resource-dir
        hostPath:
          path: "/etc/kubernetes/static-pod-resources/haproxy"
      - name: kubeconfigvarlib
        hostPath:
          path: "/var/lib/kubelet"
      - name: run-dir
        empty-dir: {}
      - name: conf-dir
        hostPath:
          path: "/etc/haproxy"
      - name: chroot-host
        hostPath:
          path: "/"
      initContainers:
      - name: verify-api-int-resolvable
        image: {{ .Images.baremetalRuntimeCfgImage }}
        command:
        - "/bin/bash"
        - "-c"
        - |
          while ! /usr/bin/curl -o /dev/null -kLfs https://api-int.{{ .DNS.Spec.BaseDomain }}:6443/healthz; do
            sleep 5
          done
        resources: {}
        terminationMessagePolicy: FallbackToLogsOnError
        volumeMounts:
        - name: chroot-host
          mountPath: "/host"
          mountPropagation: HostToContainer
        - name: kubeconfigvarlib
          mountPath: "/var/lib/kubelet"
          mountPropagation: HostToContainer
        imagePullPolicy: IfNotPresent
      containers:
      - name: haproxy
        image: {{.Images.haproxyImage}}
        env:
          - name: OLD_HAPROXY_PS_FORCE_DEL_TIMEOUT
            value: "120"
        command:
        - "/bin/bash"
        - "-c"
        - |
          #/bin/bash
          verify_old_haproxy_ps_being_deleted()
          {
            local prev_pids

            prev_pids="$1"
            sleep $OLD_HAPROXY_PS_FORCE_DEL_TIMEOUT
            cur_pids=$(pidof haproxy)

            for val in $prev_pids; do
                if [[ $cur_pids =~ (^|[[:space:]])"$val"($|[[:space:]]) ]] ; then
                   kill $val
                fi
            done
          }

          reload_haproxy()
          {
            old_pids=$(pidof haproxy)
            if [ -n "$old_pids" ]; then
                /usr/sbin/haproxy -W -db -f /etc/haproxy/haproxy.cfg  -p /var/lib/haproxy/run/haproxy.pid -x /var/lib/haproxy/run/haproxy.sock -sf $old_pids &
                #There seems to be some cases where HAProxy doesn't drain properly.
                #To handle that case, SIGTERM signal being sent to old HAProxy processes which haven't terminated.
                verify_old_haproxy_ps_being_deleted "$old_pids"  &
            else
                /usr/sbin/haproxy -W -db -f /etc/haproxy/haproxy.cfg  -p /var/lib/haproxy/run/haproxy.pid &
            fi
          }

          msg_handler()
          {
            while read -r line; do
              echo "The client send: $line"  >&2
              # currently only 'reload' msg is supported
              if [ "$line" = reload ]; then
                  reload_haproxy
              fi
            done
          }
          set -ex
          declare -r haproxy_sock="/var/run/haproxy/haproxy-master.sock"
          declare -r haproxy_log_sock="/var/run/haproxy/haproxy-log.sock"
          export -f msg_handler
          export -f reload_haproxy
          export -f verify_old_haproxy_ps_being_deleted
          rm -f "$haproxy_sock" "$haproxy_log_sock"
          socat UNIX-RECV:${haproxy_log_sock} STDOUT &
          if [ -s "/etc/haproxy/haproxy.cfg" ]; then
              /usr/sbin/haproxy -W -db -f /etc/haproxy/haproxy.cfg  -p /var/lib/haproxy/run/haproxy.pid &
          fi
          socat UNIX-LISTEN:${haproxy_sock},fork system:'bash -c msg_handler'
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: conf-dir
          mountPath: "/etc/haproxy"
          mountPropagation: HostToContainer
        - name: run-dir
          mountPath: "/var/run/haproxy"
        livenessProbe:
          initialDelaySeconds: 50
          httpGet:
            path: /haproxy_ready
            port: 9444
        terminationMessagePolicy: FallbackToLogsOnError
        imagePullPolicy: IfNotPresent
      - name: haproxy-monitor
        securityContext:
          capabilities:
            add: ["NET_ADMIN", "SYS_CHROOT"]
        image: {{ .Images.baremetalRuntimeCfgImage }}
        env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        command:
        - monitor
        - "/var/lib/kubelet/kubeconfig"
        - "/config/haproxy.cfg.tmpl"
        - "/etc/haproxy/haproxy.cfg"
        - "--api-vips"
        - "{{- range $index, $ip := onPremPlatformAPIServerInternalIPs . }}{{ if gt $index 0 }},{{end}}{{$ip}}{{end}}"
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: conf-dir
          mountPath: "/etc/haproxy"
          mountPropagation: HostToContainer
        - name: run-dir
          mountPath: "/var/run/haproxy"
        - name: resource-dir
          mountPath: "/config"
          mountPropagation: HostToContainer
        - name: chroot-host
          mountPath: "/host"
          mountPropagation: HostToContainer
        - name: kubeconfigvarlib
          mountPath: "/var/lib/kubelet"
          mountPropagation: HostToContainer
        terminationMessagePolicy: FallbackToLogsOnError
        imagePullPolicy: IfNotPresent
      hostNetwork: true
      tolerations:
      - operator: Exists
      priorityClassName: system-node-critical
    status: {}
