mode: 0644
path: {{ if isFrrEnabled . }} "/etc/kubernetes/manifests/frr.yaml" {{ else }} "/etc/kubernetes/disabled-manifests/frr.yaml" {{ end }}
contents:
  inline: |
    kind: Pod
    apiVersion: v1
    metadata:
      name: frr
      namespace: openshift-{{ onPremPlatformShortName . }}-infra
      creationTimestamp:
      deletionGracePeriodSeconds: 65
      labels:
        app: {{ onPremPlatformShortName . }}-infra-bgp
    spec:
      volumes:
      - name: resource-dir
        hostPath:
          path: "/etc/kubernetes/static-pod-resources/frr"
      - name: kubeconfig
        hostPath:
          path: "/etc/kubernetes"
      - name: script-dir
        hostPath:
          path: "/etc/kubernetes/static-pod-resources/frr/scripts"
      - name: kubeconfigvarlib
        hostPath:
          path: "/var/lib/kubelet"
      - name: conf-dir
        hostPath:
          path: "/etc/frr"
      - name: run-dir
        empty-dir: {}
      - name: chroot-host
        hostPath:
          path: "/"
      - name: nodeip-configuration
        hostPath:
          path: "/run/nodeip-configuration"
      initContainers:
      - name: verify-api-int-resolvable
        image: {{ .Images.baremetalRuntimeCfgImage }}
        command:
        - "/bin/bash"
        - "-c"
        - |
          #/bin/bash
          /host/bin/oc --kubeconfig /var/lib/kubelet/kubeconfig get nodes
        resources: {}
        volumeMounts:
        - name: chroot-host
          mountPath: "/host"
        - name: kubeconfigvarlib
          mountPath: "/var/lib/kubelet"
        imagePullPolicy: IfNotPresent
      - name: render-config-frr
        image: {{ .Images.baremetalRuntimeCfgImage }}
        command:
        - runtimecfg
        - render
        - "/etc/kubernetes/kubeconfig"
        - "--api-vips"
        - "{{- range $index, $ip := onPremPlatformAPIServerInternalIPs . }}{{ if gt $index 0 }},{{end}}{{$ip}}{{end}}"
        - "--ingress-vips"
        - "{{- range $index, $ip := onPremPlatformIngressIPs . }}{{ if gt $index 0 }},{{end}}{{$ip}}{{end}}"
        - "/config"
        - "--out-dir"
        - "/etc/frr"
        resources: {}
        volumeMounts:
        - name: kubeconfig
          mountPath: "/etc/kubernetes"
        - name: script-dir
          mountPath: "/config"
        - name: conf-dir
          mountPath: "/etc/frr"
        - name: nodeip-configuration
          mountPath: "/run/nodeip-configuration"
        imagePullPolicy: IfNotPresent
      # (emilien) We need to create /etc/frr/daemons and /etc/frr/vtysh.conf files because
      # /etc/frr is bind-mounted to generate frr.conf and frr also needs daemons configured.
      # The frr container livenessProbe needs vtysh.conf as well to be able to run.
      # There is no simple way to tell frr.conf where daemons file is and for now we will leave
      # it in /etc/frr.
      - name: copy-frr-static-configs
        image: {{ .Images.baremetalRuntimeCfgImage }}
        securityContext:
          privileged: true
        command:
        - /bin/bash
        - -c
        - |
          cat <<EOF > /etc/frr/daemons
          # generated by copy-frr-daemons container
          bgpd=yes
          ospfd=no
          ospf6d=no
          ripd=no
          ripngd=no
          isisd=no
          pimd=no
          nhrpd=no
          eigrpd=no
          sharpd=no
          pbrd=no
          bfdd=yes
          fabricd=no
          vrrpd=no
          pathd=no
          vtysh_enable=yes
          zebra_options="  -A 127.0.0.1 -s 90000000"
          bgpd_options="   -A 127.0.0.1"
          ospfd_options="  -A 127.0.0.1"
          ospf6d_options=" -A ::1"
          ripd_options="   -A 127.0.0.1"
          ripngd_options=" -A ::1"
          isisd_options="  -A 127.0.0.1"
          pimd_options="   -A 127.0.0.1"
          nhrpd_options="  -A 127.0.0.1"
          eigrpd_options=" -A 127.0.0.1"
          sharpd_options=" -A 127.0.0.1"
          pbrd_options="   -A 127.0.0.1"
          staticd_options="-A 127.0.0.1"
          bfdd_options="   -A 127.0.0.1"
          fabricd_options="-A 127.0.0.1"
          vrrpd_options="  -A 127.0.0.1"
          pathd_options="  -A 127.0.0.1"
          EOF
          touch /etc/frr/vtysh.conf
        resources: {}
        volumeMounts:
        - name: conf-dir
          mountPath: "/etc/frr"
        imagePullPolicy: IfNotPresent
      containers:
      # This container will monitor API & Ingress endpoints:
      # * if it's up, then we create the VIPs, and they'll be routed by FRR
      # * If they become down, then we ensure the VIPs aren't present
      # This container helps to do active/active for both API & Ingress traffic.
      - name: frr-manage-vips
        securityContext:
          privileged: true
        # TODO(emilien) we'll need to find another image that has curl and ip
        image: quay.io/emilien/stolostron-builder:bgp
        env:
          - name: NSS_SDB_USE_CACHE
            value: "no"
        command:
            - /bin/bash
            - -c
            - |
              #!/bin/bash
              #
              # TODO in this script:
              # * IPv6 support
              #
 
              API_VIPS="{{- range $index, $ip := onPremPlatformAPIServerInternalIPs . }}{{ if gt $index 0 }} {{end}}{{$ip}}{{end}}"
              INGRESS_VIPS="{{- range $index, $ip := onPremPlatformIngressIPs . }}{{ if gt $index 0 }} {{end}}{{$ip}}{{end}}"

              GATEWAY=$(ip route | grep default | head -n 1 | awk '{print $3}')
              NETWORK_CIDR=$(ip route | grep "proto kernel scope link src" |  awk '{print $1}')

              check_health()
              {
                  # check if the service is healthy if a curl returns 200.
                  # return 0 if healthy, 1 if not.
                  # takes one argument: the service name (api or ingress) to check (must returns 200)
                  local service=$1
                  if [ "${service}" == "api" ]; then
                    /etc/frr/chk_ocp_script_api.sh
                  elif [ "${service}" == "ingress" ]; then
                    /etc/frr/chk_ocp_script_ingress.sh
                  else
                    echo `date "+%FT%T"` "Wrong service name: ${service}"
                    return 1
                  fi
              }

              check_vip()
              {
                  # check if the VIP is assigned to this node.
                  # return 0 if it is, 1 if not.
                  # takes one argument: the VIP to check.
                  local vip=$1
                  if ip address show dev lo | grep -q $vip; then
                      echo `date "+%FT%T"` "VIP $vip is assigned to this node"
                      return 0
                  else
                      echo `date "+%FT%T"` "VIP $vip is not assigned to this node"
                      return 1
                  fi
              }
              
              # Since the VIP won't be in the same subnet of the bootstrap / master nodes, we need to configure
              # routing on the host as well. We spent a bit of time investigating it and the problem with the VIP
              # traffic going back to its source (specially for HTTPS) is that the traffic path is asymetric.
              # The destination MAC on the request was different than the source MAC the client was seeing on the reply;
              # and then packet were dropped.
              # We think that we have two options:
              # * Use a specific routing table that will route outgoing traffic coming from the VIP and supposed
              #   to go to the nodes subnet, to go through the gateway so that the traffic back follows the same
              #   path as the traffic in.
              # * For all the hosts in the nodes subnet, provide a direct route to reach the VIP.
              #
              # We decided to go with the first option (VIP traffic goes through router) because the VIP will be
              # active/active on multiple nodes (masters).
              # While this sounds complex, this comes due to abusing the deployment having different subnet IPs
              # on the same segment and expect it to not go through the router, so we need to make symetric traffic.
              create_vip_routing()
              {
                  # prepare the host to route the VIP traffic through the gateway
                  # takes one argument: the VIP to route.
                  local vip=$1
                  if ! ip rule show table 168 | grep -q $vip; then
                      echo `date "+%FT%T"` "Create ip rule for $NETWORK_CIDR in table 168"
                      ip rule add from $vip to $NETWORK_CIDR lookup 168
                  fi
                  if ! ip route show table 168 | grep -q $GATEWAY; then
                      echo `date "+%FT%T"` "Create default route for table 168 via $GATEWAY"
                      ip route add default via $GATEWAY table 168
                  fi
              }
              
              create_vip()
              {
                  # create the VIP on the node.
                  # takes one argument: the VIP to create.
                  local vip=$1
                  if ! ip address show dev lo | grep -q $vip; then
                    echo `date "+%FT%T"` "Creating VIP $vip"
                    ip address add $vip dev lo
                  fi
              }
              
              delete_vip() {
                  # delete the VIP on the node.
                  # takes one argument: the VIP to delete.
                  local vip=$1
                  if ip address show dev lo | grep -q $vip; then
                    echo `date "+%FT%T"` "Deleting VIP $vip"
                    ip address del $vip dev lo
                  fi
              }
                
              while :; do
                  VIP_NETMASK=32

                  if check_health "api"; then
                    echo `date "+%FT%T"` "Healthcheck for API OK"
                    for vip in $API_VIPS; do
                      create_vip $vip/$VIP_NETMASK
                      create_vip_routing $vip
                    done
                  else
                    echo `date "+%FT%T"` "Healthcheck for API not OK"
                    for vip in $API_VIPS; do
                      delete_vip $vip/$VIP_NETMASK
                    done
                  fi

                  if check_health "ingress"; then
                    echo `date "+%FT%T"` "Healthcheck for Ingress OK"
                    for vip in $INGRESS_VIPS; do
                      create_vip $vip/$VIP_NETMASK
                      create_vip_routing $vip
                    done
                  else
                    echo `date "+%FT%T"` "Healthcheck for Ingress not OK"
                    for vip in $INGRESS_VIPS; do
                      delete_vip $vip/$VIP_NETMASK
                    done
                  fi
                  sleep 2
              done
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: conf-dir
          mountPath: "/etc/frr"
        terminationMessagePolicy: FallbackToLogsOnError
        imagePullPolicy: IfNotPresent
      - name: frr
        # change that, see https://github.com/metallb/metallb/blob/350855a42d4d2bd510c9cdd6d484d49224879436/config/frr/speaker-patch.yaml#L55-L57
        securityContext:
          privileged: true
        image: quay.io/emilien/frr:latest
        command:
            - /bin/bash
            - -c
            - |
              #/bin/bash
              
              start_frr()
              {
                /usr/libexec/frr/frr start
              }

              stop_frr()
              {
                [ -r /var/run/frr/watchfrr.pid ] && /usr/libexec/frr/frr stop
              }

              reload_frr()
              {
                stop_frr
                start_frr
              }

              msg_handler()
              {
                while read -r line; do
                  echo "The client sent: $line" >&2
                  # currently only 'reload' msg is supported
                  if [ "$line" = reload ]; then
                      reload_frr
                  fi
                done
              }

              set -ex
              declare -r frr_sock="/var/run/frr/frr.sock"
              export -f msg_handler
              export -f reload_frr
              export -f start_frr
              export -f stop_frr

              trap stop_frr SIGTERM

              # Here we need to wait for baremetal-runtimecfg to generate frr.conf, which can take
              # time since it needs the local kubeapi to run.
              if timeout 5m bash -c "while true; do [ -f /etc/frr/frr.conf ] && break; done"; then
                  start_frr
              else
                  echo `date "+%FT%T"` "ERROR: /etc/frr/frr.conf was not found"
                  exit 1
              fi

              rm -f "$frr_sock"
              socat UNIX-LISTEN:${frr_sock},fork system:'bash -c msg_handler'
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: conf-dir
          mountPath: "/etc/frr"
        - name: kubeconfigvarlib
          mountPath: "/var/lib/kubelet"
        - name: run-dir
          mountPath: "/var/run/frr"
        - name: chroot-host
          mountPath: "/host"
        livenessProbe:
          exec:
            command:
            - vtysh
            - -c
            - show bfd peers
          initialDelaySeconds: 20
          timeoutSeconds: 5
        terminationMessagePolicy: FallbackToLogsOnError
        imagePullPolicy: IfNotPresent
      - name: frr-monitor
        securityContext:
          privileged: true
        image: docker.io/emacchi/baremetal-runtimecfg:bgp
        env:
          - name: IS_BOOTSTRAP
            value: "no"
        command:
        - dynfrr
        - "/var/lib/kubelet/kubeconfig"
        - "/config/frr.conf.tmpl"
        - "/etc/frr/frr.conf"
        - "--api-vips"
        - "{{- range $index, $ip := onPremPlatformAPIServerInternalIPs . }}{{ if gt $index 0 }},{{end}}{{$ip}}{{end}}"
        - "--ingress-vips"
        - "{{- range $index, $ip := onPremPlatformIngressIPs . }}{{ if gt $index 0 }},{{end}}{{$ip}}{{end}}"
        resources:
          requests:
            cpu: 100m
            memory: 200Mi          
        volumeMounts:
        - name: kubeconfigvarlib
          mountPath: "/var/lib/kubelet"
        - name: resource-dir
          mountPath: "/config"
        - name: conf-dir
          mountPath: "/etc/frr"
        - name: run-dir
          mountPath: "/var/run/frr"
        - name: chroot-host
          mountPath: "/host"
        - name: nodeip-configuration
          mountPath: "/run/nodeip-configuration"
        imagePullPolicy: IfNotPresent        
      hostNetwork: true
      tolerations:
      - operator: Exists
      priorityClassName: system-node-critical
    status: {}
