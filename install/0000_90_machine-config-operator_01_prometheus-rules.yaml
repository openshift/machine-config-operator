apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: machine-config-daemon
  namespace: openshift-machine-config-operator
  labels:
    k8s-app: machine-config-daemon
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
spec:
  groups:
    - name: mcd-reboot-error
      rules:
        - alert: MCDRebootError
          expr: |
             mcd_reboot_err > 0
          labels:
            severity: critical
          annotations:
            message: "Reboot failed on {{ $labels.node }} , update may be blocked"
    - name: mcd-drain-error
      rules:
        - alert: MCDDrainError
          expr: |
            mcd_drain_err > 0
          labels:
            severity: warning
          annotations:
            message: "Drain failed on {{ $labels.node }} , updates may be blocked. For more details:  oc logs -f -n {{ $labels.namespace }} {{ $labels.pod }} -c machine-config-daemon"
    - name: mcd-pivot-error
      rules:
        - alert: MCDPivotError
          expr: |
            mcd_pivot_err > 0
          labels:
            severity: warning
          annotations:
            message: "Error detected in pivot logs on {{ $labels.node }} "
    - name: mcd-kubelet-health-state-error
      rules:
        - alert: KubeletHealthState
          expr: |
            mcd_kubelet_state > 2
          labels:
            severity: warning
          annotations:
            message: "Kubelet health failure threshold reached"
    - name: system-memory-exceeds-reservation
      rules:
        - alert: SystemMemoryExceedsReservation
          expr: |
            sum by (node) (container_memory_rss{id="/system.slice"}) > ((sum by (node) (kube_node_status_capacity{resource="memory"} - kube_node_status_allocatable{resource="memory"})) * 0.9)
          for: 15m
          labels:
            severity: warning
          annotations:
            message: "System memory usage of {{ $value | humanize }} on {{ $labels.node }} exceeds 90% of the reservation. Reserved memory ensures system processes can function even when the node is fully allocated and protects against workload out of memory events impacting the proper functioning of the node. The default reservation is expected to be sufficient for most configurations and should be increased (https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-managing.html) when running nodes with high numbers of pods (either due to rate of change or at steady state)."
    - name: master-nodes-high-memory-usage
      rules:
        - alert: MasterNodesHighMemoryUsage
          expr: |
            ((sum(node_memory_MemTotal_bytes AND on (instance) label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )) - sum(node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes AND on (instance) label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" ))) / sum(node_memory_MemTotal_bytes AND on (instance) label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )) * 100) > 90
          for: 15m
          labels:
            severity: warning
          annotations:
            message: "Memory usage of {{ $value | humanize }} on {{ $labels.node }} exceeds 90%. Master nodes starved of memory could result in degraded performance of the control plane."
    - name: containers-oom-killed-warning
      rules:
        - alert: ContainersOOMKilledWarning
          expr: |
            sum by (node) (rate(container_runtime_crio_containers_oom_total[15m])) > 1
          for: 15m
          labels:
            severity: warning
          annotations:
            message: "{{ $value | humanize }} containers have been killed on {{ $labels.node }} because they ran out of memory (OOM). Please make sure to limit the memory for the containers to not overcommit the node or try to reduce the total number of pods per node."
    - name: containers-oom-killed-critical
      rules:
        - alert: ContainersOOMKilledCritical
          expr: |
            sum by (node) (rate(container_runtime_crio_containers_oom_total[15m])) > 5
          for: 15m
          labels:
            severity: critical
          annotations:
            message: "{{ $value | humanize }} containers have been killed on {{ $labels.node }} because they ran out of memory (OOM). Please make sure to limit the memory for the containers to not overcommit the node or try to reduce the total number of pods per node."
